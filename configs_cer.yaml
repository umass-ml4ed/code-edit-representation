##################################################
# exp_opts
##################################################
seed: 0
exp_name: 'cer' 
use_neptune: true
save_model: true
testing: false # only use a very small portion of the dataset for testing purposes
testing_size: 8
neptune_project: 'hhjami/Code-Edit-Representation' # change to your own "username/projectname"
log_train_every_itr: 100
model_save_dir: 'checkpoints'
use_cuda: true
# allowed_problem_list: ['12', '17', '21'] # only if else related problems
# allowed_problem_list: ['34', '37', '39', '40'] # string problems
# allowed_problem_list: ['46', '71'] # array problems requiring loops
allowed_problem_list: ['1','3','5', '12', '13', '17', '20', '21', '22', '24', '25', '34', '37', '39', '40', '46', '71'] # all problems
##################################################
# debug_opts
##################################################
verbose: true # shows debug outputs. individual flags below control which of them you want to show. setting this false, will stop printing all debug outputs
show_loss_at_every_epoch: true # shows the training loss at every epoch
show_accuracy_at_every_epoch: true # shows the accuracy for training, validation and testing at every epoch
##################################################
# data_opts
##################################################
data_path: 'data/dataset.pkl'
test_size: 0.2 # percentage of test dataset
max_len: 200 # maximum number of submission per student 
true_false_ratio: 1 # ratio of false to true samples, multiplier of the number of true
##################################################
# model_opts
##################################################
model_name: 't5-base'
model_inp_dim: 768 # Fixed at 768 as it's the output dimension of t5-base, codet5-base
# model_name: 't5-large'
# model_inp_dim: 1024 # Fixed at 1024 as it's the output dimension of t5-large, codet5-large
# model_name: 'Salesforce/codet5-large'
code_change_vector_size: 128
##################################################
# train_opts
##################################################
epochs: 2
batch_size: 8
lr: 1e-4
lr_pretrained_encoder: 0
lr_fc_edit_encoder: 1e-3
use_scheduler: true
warmup_ratio: 0.1
loss_fn: 'ContrastiveLoss' # ContrastiveLoss or NTXentLoss or TripletLoss or CosineSimilarityLoss or MultipleNegativesRankingLoss
margin: 1.0
accumulation_steps: 1
